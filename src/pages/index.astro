---
import Layout from '../layouts/Layout.astro';
import News from '../components/News/News.jsx';
import Swiper from '../components/Swiper/swiper.astro';
import Blob from '../components/Blob/blob.astro';
import Button from '../components/ui/Button.astro';
import SectionHeader from '../components/ui/SectionHeader.astro';
---

<Layout title="MLL Lab — Home">
	<main class="bg-background-light">
		<!-- Hero Section -->
		<section class="py-10 md:py-16">
			<div class="container mx-auto px-4">
				<div class="text-center">
					<h1 class="text-4xl md:text-7xl font-heading font-bold mb-6 text-secondary-900">
						<span class="text-primary-600">MLL</span> Lab
						<br>
						<span class="text-primary-600">M</span>achine <span class="text-primary-600">L</span>earning and <span class="text-primary-600">L</span>anguage 
					</h1>
					
					<p class="mx-auto max-w-3xl text-xl font-medium text-secondary-600 mb-8">
						We develop intelligent language + X (vision, robotics, etc) models that reason, plan, and interact with the physical world. 
						<!-- By integrating vision, robotics, and other modalities, we build trustworthy foundation models with spatial intelligence and compositional reasoning. -->
					</p>
					
					<div class="relative mb-8">
						<Blob />
						<div class="z-10 relative">
							<!-- <a href="https://github.com/RAGEN-AI/RAGEN" class="text-primary-600 font-medium hover:underline text-lg inline-flex items-center">
								<b>Announcing RAGEN:</b>&nbsp;Training RL Agents   
							</a> -->
							
							<div class="flex flex-col sm:flex-row justify-center gap-4 mt-6">
								<Button href="/joinus" variant="primary" size="lg">
									Join Us 
								</Button>
								
								<a href="https://ragen-ai.github.io" class="z-10 inline-flex items-center text-primary-600 hover:underline font-medium self-center">
									Announcing&nbsp;<b>RAGEN</b>:&nbsp;Training RL Agents - Github ⭐️ 2.3k
									<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-5 h-5 ml-1">
										<path stroke-linecap="round" stroke-linejoin="round" d="m8.25 4.5 7.5 7.5-7.5 7.5" />
									</svg>
								</a>
							</div>
						</div>
					</div>
				</div>
				
				<!-- Projects Carousel Section -->
				<div id="about" class="mt-6">
					<SectionHeader 
						title="" 
						subtitle=""
						centered={true}
						className="mb-4"
					/>
					<Swiper />
				</div>
			</div>
		</section>
		
		<!-- News Section -->
		<section id="news" class="py-10 bg-secondary-50 mt-6">
			<div class="container mx-auto px-4">
				<SectionHeader 
					title="Latest News" 
					centered={true}
					className="mb-6"
					titleClassName="text-primary-600"
				/>
				
				<div class="w-full">
					<div class="bg-white shadow-medium rounded-xl border border-secondary-100 p-8">
						<News client:load />
					</div>
				</div>
			</div>
		</section>
		
		<!-- Selected Publications Section -->
		<section id="publications" class="py-10 bg-background-light mt-6">
			<div class="container mx-auto px-4">
				<SectionHeader 
					title="Selected Publications" 
					centered={true}
					className="mb-6"
					titleClassName="text-primary-600"
				/>
				
				<div class="w-full">
					<div class="bg-white shadow-medium rounded-xl border border-secondary-100 p-8">
						<div class="space-y-4 text-secondary-700 text-sm leading-relaxed">
														<br>
							<!-- ### Books -->
							<!-- <br> -->
							<!-- **Advances in Information Access** <br> -->
							<!-- Sha Li, Manling Li, Heng Ji<br>  -->
							<!-- **Ongoing (World Scientifc Publishing)**  <br> -->
							<!-- ### * denotes equal contribution, <sup>†</sup> denotes mentored students -->
							<!-- <br> -->
							<p><strong>RAGEN (RL-Agent): Training Agents by Reinforcing Reasoning</strong> [<a href='https://ragen-ai.github.io'>Website</a>][<a href='https://arxiv.org/abs/2504.20073'>PDF</a>][<a href='http://github.com/ZihanWang314/ragen'>Code</a>][<a href='https://wandb.ai/zihanwang-ai-northwestern-university/ragen_latest/reports/RAGEN-Full-Experiments--VmlldzoxMjQzOTM1OA?accessToken=yzg8k9f70uleqb7ukisej6qj19kas8q2l9bvyig0hp02gwu2yv3ra4x15iyegqmw'>Experimental Logs</a>][<a href='https://x.com/wzihanw/status/1884092805598826609'>td;lr</a>]    <br>
							Zihan Wang*, Kangrui Wang*, Qineng Wang*, Pingyue Zhang*, Linjie Li*, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, Manling Li    <br>
							<strong><span style="color:#8C0000">Best Poster Award</span> at MMLS 2025 (Midwest Machine Learning Symposium)</strong>  <br>
							<strong>2.3k+ Github Stars, Featured by <a href='https://www.mittrchina.com/news/detail/14715'>MIT Tech Review</a>, <a href='https://lambda.ai/blog/agent-training-on-lambda-with-dstack-and-ragen'>Lambda Partner Spotlight</a>, <a href='https://venturebeat.com/ai/former-deepseeker-and-collaborators-release-new-method-for-training-reliable-ai-agents-ragen'>VentureBeat</a>, <a href='https://medium.com/%40tech.dose/training-ai-agents-that-dont-fall-apart-how-ragen-and-starpo-are-solving-the-echo-trap-c149b23248fa'>Medium</a>, <a href='https://www.artificialintelligence-news.com/news/ragen-ai-framework-tackles-llm-agent-instability'>AI News</a>, <a href='https://www.marktechpost.com/2025/01/31/meet-ragen-framework-the-first-open-source-reproduction-of-deepseek-r1-for-training-agentic-models-via-reinforcement-learning'>MarkTechPost</a>, <a href='https://businessleadersreview.com/'>Business Leaders Review</a>, etc.</strong>  <br></p>
							<p><strong>VAGEN: Reinfocing World Model Reasoning for Multi-Turn VLM Agents</strong> [<a href=''>PDF</a>][<a href='https://mll-lab.notion.site/vagen'>Blog</a>][<a href='https://github.com/RAGEN-AI/VAGEN'>Code</a>][<a href='https://x.com/ManlingLi_/status/1904561315114278969'>td;lr</a>]    <br>
							Kangrui Wang*, Pingyue Zhang*, Zihan Wang*, Yaning Gao*, Linjie Li*, Qineng Wang, Chi Wan, Hanyang Chen, Yiping Lu, Zhengyuan Yang, Lijuan Wang, Ranjay Krishna, Jiajun Wu, Li Fei-Fei, Yejin Choi, Manling Li    <br>
							<strong>NeurIPS 2025</strong>  <br></p>
							<!-- <span style="color: grey; font-size: 0.9em">***Open-source Project:** VAGEN is a multi-turn reinforcement learning framework designed specifically for training VLM Agents. VAGEN leverages the world model reasoning to efficiently train VLMs for visual agentic tasks.* <br>
							</span> -->
							<p><strong>Exploring Diffusion Transformer Designs via Grafting</strong> [<a href='https://grafting.stanford.edu'>Website</a>][<a href='https://arxiv.org/abs/2506.05340'>PDF</a>][<a href='https://www.liquid.ai/research/exploring-diffusion-transformer-designs-via-grafting'>Blog</a>][<a href='https://github.com/keshik6/grafting'>Code</a>][<a href='https://x.com/keshigeyan/status/1932490230290108444'>td;lr</a>] <br>
							Keshigeyan Chandrasegaran*, Michael Poli*, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei <br>
							<strong>NeurIPS 2025</strong><br>
							<span style="color:#8C0000"><strong>Oral (Top 0.36%)</strong></span>  <br></p>
							<p><strong>Spatial Mental Modeling from Limited Views</strong> [<a href='https://mll-lab-nu.github.io/mind-cube'>Website</a>][<a href='https://arxiv.org/abs/2506.21458'>PDF</a>][<a href='https://huggingface.co/datasets/MLL-Lab/MindCube'>Data</a>][<a href='https://github.com/mll-lab-nu/MindCube'>Code</a>][<a href='https://x.com/ManlingLi_/status/1939760677133987952'>td;lr</a>] <br>
							Qineng Wang*, Baiqiao Yin*, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Jiajun Wu+, Li Fei-Fei+, Manling Li+<br>
							<strong><span style="color:#8C0000">The Best of ICCV 2025</span>, featured by Voxel 51</strong><br>
							<strong><span style="color:#8C0000">Spotlight</span> at ICCV 2025 Workshop on Structural Priors for Vision</strong>  <br></p>
							<p><strong>ROSETTA: Constructing Code-Based Reward from Unconstrained Language Preference</strong> [<a href='https://sanjanasrivastava.github.io/rosetta-project/'>Website</a>][<a href='https://github.com/StanfordVL/rosetta/blob/main/preprint.pdf'>PDF</a>][<a href='https://github.com/StanfordVL/rosetta'>Data</a>][<a href='https://github.com/StanfordVL/rosetta'>Code</a>][<a href='https://x.com/sanjana__z/status/1937742849807892836'>td;lr</a>] <br>
							Sanjana Srivastava*, Kangrui Wang*, Yung-Chieh Chan*, Tianyuan Dai, Manling Li, Ruohan Zhang, Mengdi Xu, Jiajun Wu, Li Fei-Fei <br>
							<strong><span style="color:#8C0000">Best Paper Award</span> at RSS 2025 on Continual Robot Learning from Humans</strong>  <br></p>
							<p><strong>EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</strong> [<a href='https://embodiedbench.github.io/'>Website</a>][<a href='https://arxiv.org/abs/2502.09560'>PDF</a>][<a href='https://embodiedbench.github.io/'>Code</a>] <br>
							Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang <br>
							<strong>ICML 2025</strong><br>
							<span style="color:#8C0000"><strong>Oral (Top 1%)</strong></span>  <br></p>
							<p><strong>Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas</strong> [<a href='https://arxiv.org/abs/2503.01773'>PDF</a>][<a href='https://github.com/shiqichen17/AdaptVis'>Code</a>][<a href='https://github.com/shiqichen17/AdaptVis'>Data</a>] <br>
							Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, Manling Li  <br>
							<strong>ICML 2025</strong>  <br></p>
							<p><strong>Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging</strong> [<a href=''>Website</a>][<a href=''>PDF</a>][<a href=''>Code</a>][<a href=''>Data</a>]  <br>
							Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, Junxian He <br>
							<strong>ICML 2025</strong>  <br></p>
							<p><strong>SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering</strong> [<a href='https://xhguo7.github.io/SyncMind'>Website</a>][<a href='https://arxiv.org/abs/2502.06994'>PDF</a>][<a href='https://github.com/xhguo7/SyncMind'>Code</a>][<a href='https://huggingface.co/datasets/xuehang/SyncBench'>Data</a>]  <br>
							Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, Heng Ji <br>
							<strong>ICML 2025</strong>  <br></p>
							<p><strong>T*: Re-thinking Temporal Search for Long-Form Video Understanding</strong> [<a href='https://longvideohaystack.github.io'>Website</a>][<a href=''>PDF</a>][<a href='https://huggingface.co/LVHaystack'>Data</a>][<a href='https://github.com/LongVideoHaystack/TStar'>Code</a>] <br>
							Jinhui Ye*, Zihan Wang*, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, Manling Li<br>
							<strong>CVPR 2025</strong><br>
							<span style="color:#8C0000"><strong>Oral</strong></span> at ICCV 2025 Workshop on Long Multi-Scene Video Foundations <br></p>
							<p><strong>LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models</strong> [<a href='https://ai.stanford.edu/~sunfanyun/layoutvlm/'>Website</a>][<a href='https://arxiv.org/abs/2412.02193'>PDF</a>][<a href='https://github.com/sunfanyunn/LayoutVLM'>Code</a>] <br>
							Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, Jiajun Wu <br>
							<strong>CVPR 2025</strong>  <br></p>
							<p><strong>Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models</strong> [<a href='https://arxiv.org/abs/2403.17359'>PDF</a>] <br>
							Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu<br>
							<strong>ICLR 2025</strong>  <br></p>
							<p><strong>Visually Descriptive Language Modeling for Vector Graphics Reasoning</strong> [<a href='https://arxiv.org/abs/2404.06479'>PDF</a>][<a href='https://mikewangwzhl.github.io/VDLM'>Website</a>][<a href='https://github.com/MikeWangWZHL/VDLM'>Code</a>]  <br>
							Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, Heng Ji<br>
							<strong>TMLR</strong>  <br></p>
							<p><strong>The Law of Knowledge Overshadowing: Towards Understanding, Predicting and Preventing LLM Hallucination</strong> [<a href='https://arxiv.org/abs/2407.08039'>PDF</a>] <br>
							Yuji Zhang, Sha Li, Cheng Qian, Jiateng Liu, Pengfei Yu, Chi Han, Yi Fung, Kathleen McKeown, ChengXiang Zhai, Manling Li, Heng Ji<br>
							<strong>ACL 2025 Findings</strong>  <br></p>
							<!-- **ACLED-DS: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World** [<a href='https://arxiv.org/abs/2506.00980'>PDF</a>] <br>
							Sina Semnani, Pingyue Zhang, Wanyue Zhai, Haozhuo Li, Ryan Beauchamp, Trey Billing, Katayoun Kishi,  Manling Li, Monica
							Lam<br> 
							**ACL 2025 Findings**  <br> -->
							<p><strong>Chain-of-Experts: Unlocking the Communication Power of MoEs</strong> [<a href='https://arxiv.org/abs/2506.18945'>PDF</a>][<a href='https://sandy-server-87f.notion.site/Chain-of-Experts-Unlocking-the-Communication-Power-of-MoEs-1ab9bb750b7980048d43e6aab3537cea'>Blog</a>][<a href='https://github.com/ZihanWang314/coe'>Code</a>][<a href="https://x.com/ManlingLi_/status/1896642207903629790">td;lr</a>]    <br>
							Zihan Wang, Rui Pan,  Jiarui Yao, Róbert Csordás, Linjie Li, Lu Yin,  Jiajun Wu, Tong Zhang, Manling Li, Shiwei Liu    <br></p>
							<!-- **Foundation Models Meet Embodied Agents** [<a href='https://foundation-models-meet-embodied-agents.github.io'>Website/Slides/Videos</a>] <br>
							Manling Li, Yunzhu Li, Jiayuan Mao, Wenlong Huang<br> 
							**AAAI 2025: Tutorial**  <br>
							**NAACL 2025: Tutorial**  <br>
							**ICCV 2025: Tutorial**  <br> -->
							<p><strong>Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</strong> [<a href='https://embodied-agent-interface.github.io/'>Website</a>][<a href='https://arxiv.org/abs/2410.07166'>PDF</a>][<a href='https://github.com/embodied-agent-eval/embodied-agent-eval'>Code</a>][<a href='https://huggingface.co/datasets/Inevitablevalor/EmbodiedAgentInterface'>Data</a>][<a href='https://hub.docker.com/r/jameskrw/eai-eval'>Docker</a>][<a href='https://pypi.org/project/eai-eval/'>PyPi</a>][<a href='https://embodied-agent-eval.readthedocs.io/en/latest/#'>Doc</a>]   <br>
							Manling Li<sup>*</sup>, Shiyu Zhao<sup>*</sup>, Qineng Wang<sup>*</sup>, Kangrui Wang<sup>*</sup>, Yu Zhou<sup>*</sup>, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu <br>
							<strong>NeurIPS 2024 D&amp;B Track</strong> <br>
							<span style="color:#8C0000"><strong>Oral (Top 0.6%)</strong></span> <br>
							<span style="color:#8C0000"><strong>Best Paper Award at <a href="https://socalnlp.github.io/symp24/index.html" target="_blank" rel="noopener">SoCal NLP 2024</a>, Top 0.4%</strong></span><br></p>
							<p><strong>HourVideo: 1-Hour Video-Language Understanding</strong> [<a href='https://hourvideo.stanford.edu'>Website</a>][<a href='https://arxiv.org/abs/2411.04998'>PDF</a>][<a href='https://huggingface.co/datasets/HourVideo/HourVideo'>Data</a>][<a href='https://github.com/keshik6/HourVideo'>Code</a>]<br>
							Keshigeyan Chandrasegaran, Agrim Gupta, Taran Kota, Lea M. Hadzic, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei <br>
							<strong>NeurIPS 2024 D&amp;B Track</strong>  <br></p>
							<p><strong>IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos</strong> [<a href='https://yunongliu1.github.io/ikea-video-manual/'>Website</a>][<a href='https://arxiv.org/pdf/2411.11409'>PDF</a>][<a href='https://github.com/yunongLiu1/IKEA-Manuals-at-Work'>Data</a>][<a href='https://github.com/yunongLiu1/IKEA-Manuals-at-Work'>Code</a>] <br>
							Yunong Liu, Weiyu Liu, Shubh Khanna, Cristobal Eyzaguirre, Manling Li, Juan Carlos Niebles, Vineeth Ravi, Saumitra Mishra, Jiajun Wu <br>
							<strong>NeurIPS 2024 D&amp;B Track</strong>  <br></p>
							<p><strong>LM-Steer: Word Embeddings Are Steers for Language Models</strong> [<a href='https://lm-steer.github.io'>Website</a>][<a href='https://arxiv.org/abs/2305.12798'>PDF</a>][<a href='https://github.com/Glaciohound/LM-Steer'>Code</a>][<a href='https://huggingface.co/spaces/Glaciohound/LM-Steer'>Live Demo</a>][<a href='https://lm-steer.github.io/assets/slides.pdf'>Slides</a>][<a href='https://lm-steer.github.io/assets/poster.pdf'>Poster</a>] <br>
							Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, Heng Ji<br>
							<strong>ACL 2024</strong> <br>
							(<span style="color:#8C0000"><strong>Outstanding Paper Award at ACL 2024</strong></span>)<br></p>
							<p><strong>Why Does New Knowledge Create Messy Ripple Effects in LLMs?</strong> [<a href='https://arxiv.org/abs/2407.12828'>PDF</a>] <br>
							Jiaxin Qin, Zixuan Zhang, Chi Han, Pengfei Yu, Manling Li, Heng Ji<br>
							<strong>EMNLP 2024</strong>  <br></p>
							<p><strong>Deep Concept Injection for Zero-shot Multimodal Reasoning</strong> [<a href=''>PDF</a>] <br>
							Xudong Lin, Manling Li, Richard Zemel, Heng Ji, Shih-Fu Chang<br>
							<strong>EMNLP 2024</strong>  <br></p>
							<!-- **MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders** [<a href='https://arxiv.org/pdf/2410.06845'>PDF</a>] <br>
							Cheng Li, May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang, Heng Ji <br>
							**arXiv**  <br> -->
							<!-- **Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate** [<a href='https://arxiv.org/abs/2402.07401.pdf'>PDF</a>] <br>
							Kyungha Kim*, Sangyun Lee*, Kung-Hsiang Huang*, Hou Pong Chan, Manling Li, Heng Ji<br> 
							**arXiv**  <br> -->
							<!-- **InfoPattern: Unveiling Information Propagation Patterns in Social Media** [<a href='https://arxiv.org/abs/2311.15642.pdf'>PDF</a>] <br>
							Chi Han*, Jialiang Xu*, Manling Li* , Hanning Zhang*, Tarek Abdelzaher, Heng Ji<br> 
							**arXiv**  <br> -->
							<p><strong>SmartBook: AI-Assisted Situation Report Generation</strong> [<a href='https://arxiv.org/pdf/2303.14337.pdf'>PDF</a>] <br>
							Revanth Gangi Reddy, Yi Fung, Qi Zeng, Manling Li, Zihan Wang, Paul Sullivan, Heng Ji<br>
							<strong>arXiv</strong>  <br></p>
							<p><strong>Controlling Object Existence Hallucinations in Large Vision Language Models</strong> [<a href='https://arxiv.org/pdf/2310.01779.pdf'>PDF</a>] <br>
							Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, Chunyuan Li, Manling Li<br>
							<strong>arXiv</strong>  <br></p>
							<p><strong>Event-centric Multimodal Knowledge Acquisition</strong> [<a href='uploads/ManlingLiThesis.pdf'>PDF</a>] <br>
							Manling Li<br>
							Thesis Committee: Heng Ji, Jiawei Han, Chengxiang Zhai, Shih-Fu Chang, Kyunghyun Cho<br>
							<strong>Thesis</strong> (<span style="color:#8C0000"><strong>ACL Inaugral Best Desseratation Award Honorable Mention</strong></span>)<br></p>
							<!-- **ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation** [<a href=''>PDF</a>] <br>
							Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji<br> 
							**EMNLP 2023**  <br> -->
							<!-- **Defining a New NLP Playground** [<a href=''>PDF</a>] <br>
							Sha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi Fung, Charles Yu, Joel R. Tetreault, Eduard H Hovy, Heng Ji<br> 
							**EMNLP 2023 Findings**  <br> -->
							<!-- **Knowledge-Driven Vision-Language Encoding** [<a href='https://blender.cs.illinois.edu/tutorial/KnowledgeVLP/'>Website</a>] <br>
							Manling Li, Xudong Lin, Jie Lei, Mohit Bansal, Carl Vondrick, Shih-Fu Chang, Heng Ji<br> 
							**CVPR 2023 Tutorial**  <br> -->
							<!-- **Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval** [<a href='https://openreview.net/forum?id=C9hEF7lYOSP'>PDF</a>] [<a href=''>Code</a>] <br>
							Xudong Lin, Simran Tiwari, Shiyuan Huang, Manling Li, Mike Zheng Shou, Heng Ji, Shih-Fu Chang<br> 
							**CVPR 2023** <br> -->
							<!-- **Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting** [<a href='https://blender.cs.illinois.edu/tutorial/KnowledgeVLP/'>Website</a>] <br>
							Hejie Cui, Xinyu Fang, Zihan Zhang, Ran Xu, Xuan Kan, Xin Liu, Manling Li, Yangqiu Song, Carl Yang<br> 
							**NeurIPS 2023**  <br> -->
							<!-- **Non-Sequential Graph Script Induction via Multimedia Grounding** [<a href=''>PDF</a>] <br>
							Yu Zhou<sup>†</sup>, Sha Li, Manling Li, Xudong Lin, Shih-Fu Chang, Mohit Bansal and Heng Ji<br> 
							**ACL 2023** (<sup>†</sup> denotes supervised undergraduate) <br> -->
							<!-- **A Language First Approach to Procedure Planning** [<a href=''>PDF</a>] <br>
							Jiateng Liu<sup>†</sup>, Sha Li, Zhenhailong Wang, Manling Li, Heng Ji<br> 
							**ACL 2023 Findings** (<sup>†</sup> denotes supervised undergraduate) <br> -->
							<!-- **Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification** [<a href=''>PDF</a>] <br>
							Sha Li, Ruining Zhao<sup>†</sup>, Manling Li, Heng Ji, Chris Callison-Burch and Jiawei Han<br> 
							**ACL 2023** (<sup>†</sup> denotes supervised undergraduate) <br> -->
							<!-- **Multimedia Generative Script Learning for Task Planning** [<a href=''>PDF</a>] <br>
							Qingyun Wang, Manling Li, Hou Pong Chan, Lifu Huang, Julia Hockenmaier, Girish Chowdhary and Heng Ji<br> 
							**ACL 2023 Findings**  <br> -->
							<!-- **Learning to Decompose Visual Features with Latent Textual Prompts** [<a href='https://arxiv.org/abs/2210.04287'>PDF</a>] [<a href=''>Code</a>] <br>
							Feng Wang<sup>†</sup>, Manling Li, Xudong Lin, Hairong Lv, Alexander Schwing, Heng Ji<br> 
							**ICLR 2023** (<sup>†</sup> denotes supervised undergraduate) <br> -->
							<!-- **Knowledge-Driven Vision-Language Pretraining** [<a href=''>PDF</a>] [<a href='https://blender.cs.illinois.edu/tutorial/knowledgeVLP/'>Website</a>] <br>
							Manling Li, Xudong Lin, Jie Lei, Mohit Bansal, Shih-Fu Chang, Heng Ji<br> 
							**AAAI 2023: Tutorial**  <br> -->
							<!-- **Video Event Extraction via Tracking Visual States of Arguments** [<a href='https://arxiv.org/abs/2211.01781'>PDF</a>] [<a href='https://github.com/Shinetism/VidSitu-EC'>Code</a>] <br>
							Guang Yang<sup>†</sup>, Manling Li, Jiajie Zhang, Xudong Lin, Shih-Fu Chang, Heng Ji<br> 
							**AAAI 2023** (<sup>†</sup> denotes supervised undergraduate) <br> -->
							<!-- **ADEPT: A DEbiasing PrompT Framework** [<a href='https://arxiv.org/abs/2211.05414'>PDF</a>] [<a href='https://github.com/EmpathYang/ADEPT'>Code</a>] <br>
							Ke Yang<sup>†</sup>, Charles Yu, Yi Fung, Manling Li, Heng Ji<br> 
							**AAAI 2023** (<sup>†</sup> denotes supervised undergraduate) <br> -->
							<p><strong>Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners</strong> [<a href='https://arxiv.org/abs/2205.10747'>PDF</a>] [<a href='https://github.com/MikeWangWZHL/VidIL'>Code</a>] <br>
							Zhenhailong Wang<sup>†</sup>*,Manling Li*, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji<br>
							<strong>NeurIPS'22</strong> (equal contribution) <br></p>
							<p><strong>CLIP-Event:Connecting Vision and Text with Event Structures</strong> [<a href='https://arxiv.org/abs/2201.05078'>PDF</a>] [<a href='https://github.com/limanling/clip-event'>Data</a>] [<a href='https://github.com/limanling/clip-event'>Code</a>] <br>
							Manling Li, Ruochen Xu, Shuohang Wang, Xudong Lin, Chenguang Zhu, Xuedong Huang, Heng Ji, Shih-Fu Chang<br>
							<strong>CVPR'22</strong> <br>
							(<span style="color:#8C0000"><strong>Oral, Top 4.1%</strong></span>) <br></p>
							<!-- **COVID-19 Claim Radar: A Structured Claim Extraction and Tracking System** [<a href=''>PDF</a>] [<a href='https://github.com/blender-nlp/covid-claim-radar'>Code</a>] [<a href='http://18.221.187.153/'>Demo</a>] [<a href='http://blender.cs.illinois.edu/aida/covid_claim_radar.mp4'>Video</a>] <br>
							Manling Li, Revanth Gangi Reddy, Ziqi Wang, Yi-Shyuan Chiang, Tuan M. Lai, Pengfei Yu, Zixuan Zhang,Heng Ji<br>
							**ACL'22 Demo**  <br> -->
							<!-- **Event Schema Induction with Double Graph Autoencoders** [<a href='https://aclanthology.org/2022.naacl-main.147/'>PDF</a>] [<a href='https://github.com/tracyjin/DoubleGAE'>Code</a>] <br>
							Xiaomeng Jin<sup>†</sup>, Manling Li and Heng Ji<br>
							**NAACL'22**  <br> -->
							<!-- **New Frontiers of Information Extraction** [<a href='https://aclanthology.org/2022.naacl-tutorials.3/'>PDF</a>] [<a href='https://cogcomp.seas.upenn.edu/page/tutorial.202207/'>Website</a>] [<a href='https://cogcomp.seas.upenn.edu/page/tutorial.202207/'>Slides</a>] [<a href='https://underline.io/events/325/sessions/11172/lecture/55589-t3-new-frontiers-of-information-extraction'>Videos</a>]<br>
							Muhao Chen, Lifu Huang, Manling Li, Ben Zhou, Heng Ji<br>
							**NAACL'22: Tutorial**  <br> -->
							<!-- **MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding** [<a href='https://arxiv.org/abs/2112.10728'>PDF</a>] [<a href='https://github.com/blender-nlp/MuMuQA'>Data</a>] <br>
							Revanth Gangi Reddy<sup>†</sup>, Xilin Rui<sup>†</sup>, Manling Li, Xudong Lin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mohit Bansal, Avi Sil, Shih-Fu Chang, Alexander Schwing, Heng Ji<br>
							**AAAI'22**  <br> -->
							<!-- **The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction** [<a href='https://blender.cs.illinois.edu/paper/schema2021a.pdf'>PDF</a>] [<a href='https://github.com/limanling/temporal-graph-schema'>Data</a>] <br>
							Manling Li, Sha Li, Zhenhailong Wang, Lifu Huang, Kyunghyun Cho, Heng Ji, Jiawei Han and Clare Voss<br>
							**EMNLP'21**  <br> -->
							<!-- **Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport** [<a href=''>PDF</a>] [<a href=''>Data</a>] <br>
							Manling Li, Tengfei Ma, Mo Yu, Lingfei Wu, Tian Gao, Heng Ji and Kathleen McKeown<br>
							**EMNLP'21**  <br> -->
							<!-- **Joint Multimedia Event Extraction from Video and Article** [<a href=''>PDF</a>] [<a href=''>Data</a>] <br>
							Brian Chen, Xudong Lin, Christopher Thomas, Manling Li, Shoya Yoshida, Lovish Chum, Heng Ji and Shih-Fu Chang<br>
							**EMNLP'21** Findings  <br> -->
							<!-- **Event-centric Natural Language Processing** [<a href='https://blender.cs.illinois.edu/paper/eventtutorial2021.pdf'>PDF</a>] [<a href='https://blender.cs.illinois.edu/paper/aaai_tutorial_2021_event_centric_nlu.pdf'>Slides</a>] <br>
							 Muhao Chen, Hongming Zhang, Qiang Ning, Manling Li, Heng Ji, Kathleen McKeown and Dan Roth<br>
							**ACL'21**: Tutorial.  <br> -->
							<p><strong>COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation</strong> [<a href='https://blender.cs.illinois.edu/paper/COVIDKG.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/covid19/'>Code/Data</a>]<br>
							Qingyun Wang, Manling Li, Xuan Wang, Nikolaus Parulian, Guangxing Han, Jiawei Ma, Jingxuan Tu, Ying Lin, Haoran Zhang, Weili Liu, Aabhas Chauhan, Yingjun Guan, Bangzheng Li, Ruisong Li, Xiangchen Song, Heng Ji, Jiawei Han, Shih-Fu Chang, James Pustejovsky, David Liem, Ahmed Elsayed, Martha Palmer, Jasmine Rah, Clare Voss, Cynthia Schneider, Boyan Onyshkevych <br>
							<strong>NAACL'21</strong>: System Demonstrations <br>
							(<span style="color:#8C0000"><strong>Best Demo Paper Award at NAACL2021</strong></span>)<br></p>
							<!-- **RESIN: A Dockerlized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System** [<a href='https://blender.cs.illinois.edu/paper/resin-phase1.pdf'>PDF</a>] [<a href='https://github.com/RESIN-KAIROS/RESIN-pipeline-public'>Code</a>] <br>
							 Haoyang Wen, Ying Lin, Tuan M. Lai, Xiaoman Pan, Sha Li, Xudong Lin, Ben Zhou, Manling Li, Haoyu Wang, Hongming Zhang, Xiaodong Yu, Alexander Dong, Zhenhailong Wang, Yi R. Fung, Piyush Mishra, Qing Lyu, Dídac Surís, Brian Chen, Susan W. Brown, Martha Palmer, Chris Callison-Burch, Carl Vondrick, Jiawei Han, Dan Roth, Shih-Fu Chang and Heng Ji<br>
							**NAACL'21**: System Demonstrations <br> -->
							<!-- **Event-centric Natural Language Processing** [<a href='https://blender.cs.illinois.edu/paper/eventtutorial2021.pdf'>PDF</a>] [<a href='https://blender.cs.illinois.edu/paper/aaai_tutorial_2021_event_centric_nlu.pdf'>Slides</a>] <br>
							 Muhao Chen, Hongming Zhang, Qiang Ning, Manling Li, Heng Ji and Dan Roth<br>
							**AAAI'21**: Tutorial.  <br> -->
							<p><strong>Connecting the Dots: Event Graph Schema Induction with Path Language Modeling</strong> [<a href='https://blender.cs.illinois.edu/paper/eventgraphschema2020.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/software/pathlm'>Code/Data</a>] [<a href='docs/paper237-schema-presentation.pdf'>Slides</a>] <br>
							Manling Li, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng Ji, Jonathan May, Nathanael Chambers and Clare Voss <br>
							<strong>EMNLP'20</strong>  <br></p>
							<p><strong>GAIA: A Fine-grained Multimedia Knowledge Extraction System</strong> [<a href='https://blender.cs.illinois.edu/paper/aidaacl2020demo.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/software/gaia-ie'>Code</a>] [<a href='http://blender.cs.illinois.edu/software/gaia-ie/gaia.mp4'>Video</a>]<br>
							Manling Li*, Alireza Zareian*, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare R. Voss,  Dan Napierski, Marjorie Freedman <br>
							<strong>ACL'20</strong> <br>
							(<span style="color:#8C0000"><strong>Best Demo Paper Award at ACL2020</strong></span>) <br></p>
							<p><strong>Cross-media Structured Common Space for Multimedia Event Extraction</strong> [<a href='https://blender.cs.illinois.edu/paper/multimediaspace2020.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/software/m2e2'>Code</a>] [<a href='docs/ACL20-m2e2_presentation.pdf'>Slides</a>]<br>
							Manling Li*, Alireza Zareian*, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, Shih-Fu Chang <br>
							<strong>ACL'20</strong> pp.2557–2568 <br></p>
							<p><strong>GAIA at SM-KBP 2020: A Dockerized Multi-media Multi-lingual Knowledge Extraction, Clustering, Temporal Tracking and Hypothesis Generation System</strong> [<a href='https://dsr.cise.ufl.edu/wp-content/uploads/2021/02/gaia_smkbp_2020.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2020/KBP/SM-KBP/index.html'>Project</a>] <br>
							Manling Li, Ying Lin, Tuan Manh Lai, Xiaoman Pan, Haoyang Wen, Sha Li, etc %Zhenhailong Wang, Pengfei Yu, Lifu Huang, Di Lu, Qingyun Wang, Haoran Zhang, Qi Zeng, Chi Han, Zixuan Zhang, Yujia Qin, Xiaodan Hu, Nikolaus Parulian, Daniel Campos, Heng Ji, Brian Chen, Xudong Lin, Alireza Zareian, Amith Ananthram, Emily Allaway, Shih-Fu Chang, Kathleen McKeown, Yixiang Yao, Yifan Wang, Michael Spector, Mitchell DeHaven, Daniel Napierski, Marjorie Freedman, Pedro Szekely, Haidong Zhu, Ram Nevatia, Yang Bai, Yifan Wang, Ali Sadeghian, Haodi Ma, Daisy Zhe Wang <br>
							<strong>TAC-KBP</strong>: Text Analysis Conference Knowledge Base Population Workshop 2020 <br>
							<span style="color:#8C0000"><strong>Rank 1st</strong> in the National Institute of Standards and Technology (NIST) <a href='https://tac.nist.gov/2019/SM-KBP/index.html'>Streaming Multimedia Knowledge Base Population (SM-KBP) 2020</a></span><br></p>
							<!-- **UIUC TAC2020 RUFES System Description**  [<a href='https://blender.cs.illinois.edu/paper/rufesuiuc2020.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2020/KBP/RUFES/index.html'>Project</a>] 
							<br>
							Revanth Gangi Reddy, Manling Li, Haoyang Wen and Heng Ji<br>
							**TAC-KBP**: Text Analysis Conference Knowledge Base Population Workshop 2020 <br> -->
							<p><strong>Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization</strong><br>
							[<a href='docs/multimediasummarization2019.pdf'>PDF</a>] [<a href='https://github.com/limanling/MeetingSum'>Code</a>]
							<br>
							Manling Li, Lingyu Zhang, Heng Ji, Rich Radke <br>
							<strong>ACL'19</strong>: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.2190–2196 <br></p>
							<!-- **Multilingual Entity, Relation, Event and Human Value Extraction**  [<a href='https://blender.cs.illinois.edu/paper/naacldemo2019.pdf'>PDF</a>] [<a href='https://github.com/limanling/uiuc_ie_pipeline_coarse_grained'>Code</a>] [<a href='https://youtu.be/cQPHaxGLn8k'>Video</a>] <br>
							Manling Li, Ying Lin, Joe Hoover, Spencer Whitehead, Clare Voss, Morteza Dehghani, Heng Ji <br>
							**NAACL'19**: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp.110–115 <br> -->
							<!-- **The Unobtrusive Group Interaction (UGI) Corpus** [<a href='docs/UGI.pdf'>PDF</a>] [<a href='https://sites.google.com/view/ugirpi'>Code</a>]  <br>
							Indrani Bhattacharya, Michael Foley, Ni Zhang, Tongtao Zhang, Christine Ku, Cameron Mine, Manling Li, Heng Ji, etc. <br>
							**ACM MMSys'19**: ACM Multimedia Systems Conference 2019, pp.249-254  <br> -->
							<p><strong>GAIA at SM-KBP 2019: A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System</strong> [<a href='docs/GAIA2019.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2020/SM-KBP/index.html'>Project</a>] <br>
							Manling Li, Ying Lin, Ananya Subburathinam, Spencer Whitehead, Xiaoman Pan, Di Lu, Qingyun Wang, Tongtao Zhang, Lifu Huang, Heng Ji, Alireza Zareian, Hassan Akbari, Brian Chen, Bo Wu, Emily Allaway,
							Shih-Fu Chang, Kathleen McKeown, Yixiang Yao, Jennifer Chen, Eric Berquist, Kexuan Sun, Xujun Peng, Ryan Gabbard
							Marjorie Freedman, Pedro Szekely, T.K. Satish Kumar, Arka Sadhu, Ram Nevatia, Miguel Rodriguez, Yifan Wang, Yang Bai, Ali Sadeghian, Daisy Zhe Wang <br>
							<strong>TAC-KBP</strong>: Text Analysis Conference Knowledge Base Population Workshop 2019 <br>
							<span style="color:#8C0000"><strong>Rank 1st</strong> in the National Institute of Standards and Technology (NIST) <a href='https://tac.nist.gov/2019/SM-KBP/index.html'>Streaming Multimedia Knowledge Base Population (SM-KBP) 2019</a></span><br></p>
							<!-- **A Baseline Fine-Grained Entity Extraction System for TAC-KBP2019**  [<a href='docs/UIUC_TAC_KBP2019_Fine_Grained_Entity_Extraction_System.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2019/workshop/tac2019.general.html'>Project</a>] 
							<br>
							Ying Lin, Xiaoman Pan, Manling Li, Heng Ji<br>
							**TAC-KBP**: Text Analysis Conference Knowledge Base Population Workshop 2019 <br> -->
							<!-- 2. **GAIA - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System**  [<a href='docs/GAIA.pdf'>PDF</a>] <br>
							Tongtao Zhang, Ananya Subburathinam, Ge Shi, Lifu Huang, Di Lu, Xiaoman Pan, Manling Li, Boliang Zhang, Qingyun Wang, Spencer Whitehead, Heng Ji, etc. <br>
							**TAC-KBP**: Text Analysis Conference Knowledge Base Population Workshop 2018  <br>  --> 
							<!-- **Link Prediction in Knowledge Graphs: A Hierarchy-Constrained Approach**  [<a href='https://ieeexplore.ieee.org/document/8450054'>PDF</a>] [<a href=''>Code</a>] <br>
							Manling Li, Yuanzhuo Wang, Denghui Zhang, Yantao Jia, Xueqi Cheng <br>
							IEEE Transactions on Big Data, pp.1-14 <br>
							Special Issue on "Knowledge Graphs: Techniques and Applications"   -->
							<!-- **Hierarchical Types Constrained Topic Entity Detection for Knowledge Base Question Answering**  [<a href='https://dl.acm.org/doi/abs/10.1145/3184558.3186916'>PDF</a>] <br>
							Yunqi Qiu, Manling Li, Yuanzhuo Wang, Yantao Jia, Xiaolong Jin, Xueqi Cheng <br>
							**WWW 2018**: Companion Proceedings of The Web Conference 2018, pp.35-36  (abstract paper) <br> -->
							<!-- 
							**Path-Based Attention Neural Model for Fine-Grained Entity Typing**  [<a href='docs/PAN.pdf'>PDF</a>] [<a href='https://github.com/zdh2292390/PAN'>Code</a>]<br>
							Denghui Zhang, Manling Li, Pengshan Cai, Yantao Jia,  Yuanzhuo Wang, Xueqi Cheng <br>
							**AAAI 2018**: Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pp.8179-8180 (abstract paper) <br>
							
							
							**Efficient Parallel Translating Embedding For Knowledge Graphs**  [<a href='docs/ParTransX.pdf'>PDF</a>] [<a href='https://github.com/zdh2292390/ParTrans-X'>Code</a>] <br>
							Denghui Zhang, Manling Li, Yantao Jia, Yuanzhuo Wang, Xueqi Cheng <br>
							**WI 2017**: Proceedings of IEEE/WIC/ACM International Conference on Web Intelligence, pp.460-468<br>
							
							**OpenKN at TAC KBP 2016** [<a href='docs/TAC2016_ICTCAS_OKN.pdf'>PDF</a>] [<a href='https://tac.nist.gov//2016/KBP/'>Project</a>] <br>
							Manling Li, Xinlei Chen, Yantao Jia, Yuanzhuo Wang, etc. <br> 
							**TAC-KBP**: Text Analysis Conference Knowledge Base Population Workshop 2016 <br>
							<!-- (Cold Start Entity Discovery and Linking: ranked **2nd** out of 7 teams, where in Entity Discovery, ranked **1st** out of 7 teams, and **4** measures ranked **1st** among 6 measures; Cold Start Slot Filling: ranked 9th out of 19 teams)  <br>
							--> 
							<!-- **Hierarchy-Based Link Prediction in Knowledge Graphs** [<a href='docs/hTransA.pdf'>PDF</a>] [<a href=''>Poster</a>]<br>
							Manling Li, Yantao Jia, Yuanzhuo Wang, Jingyuan Li, Xueqi Cheng. <br> 
							**WWW 2016**: Proceedings of the 25th International Conference Companion on World Wide Web, pp.77-78 (abstract paper) <br>
							
							
							**Predicting Links and Their Building Time: A Path-Based Method** [<a href='docs/TDLP.pdf'>PDF</a>] [<a href=''>Poster</a>] <br>
							Manling Li, Yantao Jia, Yuanzhuo Wang, Zeya Zhao, Xueqi Cheng. <br>
							**AAAI 2016**: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp.4228-4229 (abstract paper) <br> -->
							</div>
						
						<div class="text-center pt-4">
							<a href="/publications" class="inline-flex items-center text-primary-600 hover:text-primary-700 font-medium">
								View All Publications
								<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-4 h-4 ml-1">
									<path stroke-linecap="round" stroke-linejoin="round" d="m8.25 4.5 7.5 7.5-7.5 7.5" />
								</svg>
							</a>
						</div>
					</div>
				</div>
			</div>
		</section>
	</main>
</Layout>

<style>
	.blob {
		position: absolute;
		top: 50%;
		left: 50%;
		transform: translate(-50%, -50%);
		z-index: 0;
	}
</style>

